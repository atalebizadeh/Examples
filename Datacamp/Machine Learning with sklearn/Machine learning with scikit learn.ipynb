{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Machine learning with scikit learn.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XPPwsb_36BR",
        "colab_type": "text"
      },
      "source": [
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/datacamp/python-live-training-template/blob/master/assets/datacamp.svg?raw=True\" alt = \"DataCamp icon\" width=\"50%\">\n",
        "</p>\n",
        "<br><br>\n",
        "\n",
        "## **Machine Learning with scikit learn**\n",
        "\n",
        "Welcome to this hands-on training where you will immerse yourself in machine learning with Python. Using both `pandas` and `scikit learn`, we'll learn how to process data for machine learning and create predictions on a churn case study. In this session you will learn:\n",
        "\n",
        "- The different types of machine learning and when to use them.\n",
        "- How to apply data preprocessing for machine learning including feature engineering. \n",
        "- How to apply supervised machine learning models to generate predictions!\n",
        "\n",
        "## **The Dataset**\n",
        "\n",
        "The dataset to be used in this webinar is a CSV file named `telco.csv`, which contains data on telecom customers churning and some of their key behaviors. It contains the following columns:\n",
        "\n",
        "**Features**:\n",
        "\n",
        "- `customerID`: Unique identifier of a customer.\n",
        "- `gender`: Gender of customer.\n",
        "- `SeniorCitizen`: Binary variable indicating if customer is senior citizen.\n",
        "- `Partner`: Binary variable if customer has a partner.\n",
        "- `Dependents`: Binary variable if customer has dependent.\n",
        "- `tenure`: Number of weeks as a customer.\n",
        "- `PhoneService`: Whether customer has phone service.\n",
        "- `MultipleLines`: Whether customer has multiple lines.\n",
        "- `InternetService`: What type of internet service customer has (`\"DSL\"`, `\"Fiber optic\"`, `\"No\"`).\n",
        "- `OnlineSecurity`: Whether customer has online security service.\n",
        "- `OnlineBackup`: Whether customer has online backup service.\n",
        "- `DeviceProtection`: Whether customer has device protection service.\n",
        "- `TechSupport`: Whether customer has tech support service.\n",
        "- `StreamingTV`: Whether customer has TV streaming service.\n",
        "- `StreamingMovies`: Whether customer has movies streaming service.\n",
        "- `Contract`: Customer Contract Type (`'Month-to-month'`, `'One year'`, `'Two year'`).\n",
        "- `PaperlessBilling`: Whether paperless billing is enabled.\n",
        "- `PaymentMethod`: Payment method.\n",
        "- `MonthlyCharges`: Amount of monthly charges in $.\n",
        "- `TotalCharges`: Amount of total charges so far.\n",
        "\n",
        "**Target Variable**:\n",
        "\n",
        "- `Churn`: Whether customer `'Stayed'` or `'Churned'`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUZGmwco35DZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "6f1ce260-56f9-45be-f624-370cfe058cce"
      },
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9QtPmGc0ivB",
        "colab_type": "text"
      },
      "source": [
        "## **Data Exploration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB84Bnxj0ivD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in dataset\n",
        "telco = pd.read_csv('https://raw.githubusercontent.com/datacamp/machine-learning-with-scikit-learn-live-training/master/data/telco_churn.csv', index_col = \"Unnamed: 0\")\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1xRb-Ar0ivJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print header\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsmNCE8U0iva",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print info\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK9GKWFq0ivo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Take a look at unique values in telco\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNQdigfw0ivz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unique values of internet service"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IWmBMGQ9yqi",
        "colab_type": "text"
      },
      "source": [
        "The **null model** is a model of reference to use for classification accuracy - where the  **null accuracy** is the accuracy of the model if we always choose the most frequent class *(or outcome)*. Accuracy is determined here by the following:\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "$$\\large{accuracy = \\frac{\\# \\space times \\space model \\space is \\space right}{total \\space number \\space of \\space predictions}}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnasvjxF0iv9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the null model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyebp5IvAdGo",
        "colab_type": "text"
      },
      "source": [
        "$$\\large{null \\space accuracy = \\frac{\\# \\space times \\space model \\space predicted \\space \"Stayed\"}{total \\space number \\space of \\space predictions}}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd6E2YbQ0iwB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the null model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYynPBTc-vYU",
        "colab_type": "text"
      },
      "source": [
        "In this particular instance, the null model (always predicting `\"Stayed\"`) is 73.4% - and any meaningful model that improves performance will have to break that accuracy score. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot-F9i0l6NLb",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "<center><h1> Q&A 1</h1> </center>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJc5u_yP0iwI",
        "colab_type": "text"
      },
      "source": [
        "## **Data Cleaning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzqvJShMA-TE",
        "colab_type": "text"
      },
      "source": [
        "**Task 1: Dropping** `customerID` **column**\n",
        "\n",
        "To drop a column from a DataFrame - we can use the `.drop()` method alongside the following arguments:\n",
        "\n",
        "- Name of `column` dropped - in this example `'customerID'`\n",
        "- `axis`: Whether to drop row (`0`), or column (`1`).\n",
        "- `inplace`: Boolean whether to drop in place and overwrite change in DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqLtk5Et0iwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop customer ID column\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8OX7u2eBHQ1",
        "colab_type": "text"
      },
      "source": [
        "**Task 2: Converting** `TotalCharges` **column**\n",
        "\n",
        "To convert a column from string to numeric - we can use the `pd.to_numeric()` function - which takes the following arguments:\n",
        "\n",
        "- Name of `column` to convert - in this example `'TotalCharges'`\n",
        "- `errors`: Whether to `'raise'` an error if cannot convert or to `'coerce'` it to `NaN`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utp87oQf0iwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert TotalCharges to numeric\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SginoXnb0iwT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print info\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMPV4-d50iwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print # of missing values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoNNRE7M0iwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize distribution of TotalCharges\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91WycNU30iwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get distribution of TotalCharges\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK3bVIjfD_GP",
        "colab_type": "text"
      },
      "source": [
        "As a reminder, the `.loc[]` method lets us slice a DataFrame by a group of rows or columns by labels or boolean arrays - meaning we can subset a DataFrame `df` as such:\n",
        "\n",
        "```\n",
        "df.loc[row condition, column label]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OykkW3l80iwj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Replace NA of TotalCharges with median\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UkD8u1N0iwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make sure no more \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNXC7kmfEap4",
        "colab_type": "text"
      },
      "source": [
        "**Task 3: Collapse** values of `InternetService` **column**\n",
        "\n",
        "To replace the values in of a column `col_A` in a DataFrame `df` - we can use the `.replace()` method which takes in a dictionary mapping the `old_value` to the `new_value` as such:\n",
        "\n",
        "```\n",
        "df['col_A'] = df['col_A'].replace({old_value : new_value})\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7lMvuss0iwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Collapse 'dsl' into 'DSL'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Dp3t62t6klv",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "<center><h1> Q&A 2</h1> </center>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0yg1rYr0iws",
        "colab_type": "text"
      },
      "source": [
        "## **Exploratory Analysis for Machine Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFCOeFKaFdb8",
        "colab_type": "text"
      },
      "source": [
        "In order to understand which models have predictive power, which variables to use in feature engineering, and to build a common sense understanding of what is driving churn to understand results, it essential to explore the data and observe how **features** interact with the **target** variable. \n",
        "\n",
        "There are broadly 3 types of data: \n",
        "\n",
        "- Continous _(e.g. age)_ data. \n",
        "- Categorical data _(e.g. marriage status)_. \n",
        "- Other *(e.g. image, tweets, etc...)*\n",
        "\n",
        "Let's visualize how continous and categorical data in `telco` behave with `Churn`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmxBKliY0iw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Grab a look at the header\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uNWaUFiGsMP",
        "colab_type": "text"
      },
      "source": [
        "> *A note on list comprehensions:*\n",
        ">\n",
        "> List comprehensions provide an elegant way to iteratively produce lists without using a traditional `for` loop. For example, here's how we can create a list of numbers from 0 to 9:\n",
        ">\n",
        "> ```\n",
        "> my_list = [i for i in range(0,10)]\n",
        ">\n",
        "> print(my_list)\n",
        ">\n",
        "> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "```\n",
        ">\n",
        "> It is also possible to add an `if` statement to create a condition while creating the list - here is an example where we create a list of numbers bigger than 3 from values ranging 0 to 9:\n",
        ">\n",
        ">```\n",
        "># Create a list of the doubles of values from 0 to 9\n",
        ">my_list = [i for i in range(0,10) if i > 3]\n",
        ">\n",
        ">print(my_list)\n",
        ">\n",
        ">[4, 5, 6, 7, 8, 9]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hWyT3ncpq55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get dtypes of column\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3znf0qLD0iw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get all features\n",
        "\n",
        "\n",
        "# Get all categorical features\n",
        "\n",
        "\n",
        "# Get all numeric columns\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKwQt-V10iw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print them out and make sure\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etRPlz5mrpnL",
        "colab_type": "text"
      },
      "source": [
        "> #### **Data Visualization Refresher** \n",
        "> \n",
        "> A `matplotlib` visualization is made of 3 components:\n",
        "> - A **figure** which houses in one or many subplots (or axes).\n",
        "> - The **axes** objects ~ the subplots within the figure.\n",
        "> - The plot inside each subplot or axes.\n",
        ">\n",
        "> We can generate a figure with subplots using the following function:\n",
        ">\n",
        "> `fig, axes = plt.subplots(nrow, ncol)`\n",
        "> \n",
        "> <p align=\"left\">\n",
        "<img src=\"https://github.com/adelnehme/intro-to-data-visualization-Python-live-training/blob/master/images/subplots.gif?raw=true\" width=\"55%\">\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kvRhH6VvSYP",
        "colab_type": "text"
      },
      "source": [
        "#### **Visualizing target variable relationship with categorical features**\n",
        "\n",
        "To visualize the count of different categorical values by `Churn`, we can use the `sns.countplot(x, hue, data, ax)` function which takes in:\n",
        "- `x`: The column name being counted.\n",
        "- `hue`: The column name used for grouping the data.\n",
        "- `data`: The DataFrame being visualized.\n",
        "- `ax`: Which axes in the figure to assign the plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz-uBHw50iw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting aesthetics for better viewing\n",
        "plt.rcParams[\"axes.labelsize\"] = 5\n",
        "sns.set(font_scale=5) \n",
        "\n",
        "# Create figure and axes\n",
        "fig, axes = plt.subplots(5, 3, figsize = (100, 100))\n",
        "\n",
        "# Iterate over each axes, and plot a countplot with categorical columns\n",
        "for ax, column in zip(axes.flatten(), categorical):\n",
        "    \n",
        "    # Create countplot\n",
        "    \n",
        "    \n",
        "    # Set the title of each subplott\n",
        "    \n",
        "\n",
        "    # Improve legends\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    fig.legend(handles, labels, loc='right', fontsize = 48)\n",
        "    ax.get_legend().remove()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyFXV1bnumI0",
        "colab_type": "text"
      },
      "source": [
        "**Observation 6:** Gender seems to have a 50-50 split and values and does not affect Churn.\n",
        "\n",
        "**Observation 7:** `Fiber optic` Internet Service seems to be a driver of Churn.\n",
        "\n",
        "**Observation 8:** `OnlineBackup`, `DeviceProtection`, `TechSupport` and `OnlineSecurity` users tend to churn less."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ7wyQbrvOG7",
        "colab_type": "text"
      },
      "source": [
        "#### **Visualizing target variable relationship with continuous features**\n",
        "\n",
        "A great way to observe the differences between two groups (or categories) of data according to a numeric value is a boxplot, which visualizes the following:\n",
        "\n",
        "<p align=\"left\">\n",
        "<img src=\"https://github.com/adelnehme/intro-to-data-visualization-Python-live-training/blob/master/images/boxplot.png?raw=true\" alt = \"DataCamp icon\" width=\"80%\">\n",
        "</p>\n",
        "\n",
        "It can be visusalized as such:\n",
        "\n",
        "- `sns.boxplot(x=, y=, data=)`\n",
        "  - `x`: Categorical variable we want to group our data by.\n",
        "  - `y`: Numeric variable being observed by group.\n",
        "  - `data`: The DataFrame being used.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTQU6my40ixF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting aesthetics for better viewing\n",
        "plt.rcParams[\"axes.labelsize\"] = 1\n",
        "sns.set(font_scale=1) \n",
        " \n",
        "# Create figure and axes\n",
        "fig, axes = plt.subplots(1, 3, figsize = (20, 8))\n",
        "\n",
        "# Iterate over each axes, and plot a boxplot with numeric columns\n",
        "for ax, column in zip(axes.flatten(), numeric):\n",
        "    \n",
        "    # Create a boxplot\n",
        "    \n",
        "    \n",
        "    # Set title\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64JAbMPlwRnx",
        "colab_type": "text"
      },
      "source": [
        "**Observation 8:** Higher monthly charges tend to be related to Churn.\n",
        "\n",
        "**Observation 9:** Tenure may seem predictive, but it could very well much be that churners have low tenure by nature because they are churning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwY6BYxn6osc",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "<center><h1> Q&A 3</h1> </center>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP69JRvx0ixL",
        "colab_type": "text"
      },
      "source": [
        "## **Data pre-processing for machine learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXDndg5fxU10",
        "colab_type": "text"
      },
      "source": [
        "Many machine learning algorithms require data to be processed before being passed into an algorithm first - dependent on whether data is numeric or categorical, the processing strategy is different.\n",
        "\n",
        "**Continuous or numeric data**\n",
        "\n",
        "Many machine learning models make assumptions about the distribution of numeric features when modeling (most commonly data is assumed to be normally distributed). Also, many numeric columns have different scales _(e.g. Age vs Salary)_. \n",
        "\n",
        "A common way to process numeric columns is through **Standardization** - where we substract their mean and divide by their standard deviation so that their mean becomes centered around 0 and have a standard deviation of 1 :\n",
        "\n",
        "$$\\large{x_{scaled} = \\frac{x - mean}{std}}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "<p align=\"left\">\n",
        "<img src=\"https://github.com/datacamp/machine-learning-with-scikit-learn-live-training/blob/master/assets/standard_scaler.gif?raw=true?resize\" width=\"45%\">\n",
        "</p>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "We can do this easily in `sklearn` by using the `StandardScaler()` function. Many operations in `sklearn` fit the following `.fit()` $\\rightarrow$ `.transform()` paradigm and `StandardScaler()` is no different:\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit on data\n",
        "scaler.fit(df[my_column])\n",
        "\n",
        "# Transformed\n",
        "column_scaled = scaler.transform(df[my_column])\n",
        "\n",
        "# Replace column\n",
        "df[my_column] = column_scaled\n",
        "```\n",
        "\n",
        "However, it is very important to **first split** your data before scaling your features since we do not want to scale our data according to the distribution of both the training data and test data. Failing to do so results in **data leakage** and could lead to \"too good to be true\" results on testing data with relatively weaker results on unseen data. \n",
        "\n",
        "<font color=00AAFF>Ideally, scalers should be fit on **training data only** - and be used to transform both training and testing data.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g4VgqJm0ixR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split data between X and label\n",
        "X = telco[features]\n",
        "y = telco['Churn'].replace({'Stayed': 0, 'Churned':1})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DQPNtFc0ixU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into train test splits\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Os66Te3M0ixY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import StandardScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Intialize a scaler\n",
        "\n",
        "\n",
        "# Fit on training data\n",
        "\n",
        "\n",
        "# Transform training and test data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKBTS0PS0ixe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Replace columns in training and testing data accordingly\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf6Erb-M3xxE",
        "colab_type": "text"
      },
      "source": [
        "**Categorical data**\n",
        "\n",
        "While categorical variables like country, marriage status and more are easily interpretable by humans - they need to be properly encoded to be understood by machine learning algorithms. We will be using dummy encoding *(highly similar to one-hot encoding)* where categorical variables are converted to binary (`1`,`0`) columns to indicate whether they have a certain value or not. Note that, dummy encoding generates `n-1` categories. Using a country example - `0` on all columns encodes it as France.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/datacamp/machine-learning-with-scikit-learn-live-training/blob/master/assets/onehot_dummy.gif?raw=true\" width=\"80%\">\n",
        "</p>\n",
        "\n",
        "Using dummy encoding in `pandas` is actually very easy - we can use the `pd.get_dummies()` function which takes:\n",
        "\n",
        "- The DataFrame being converted.\n",
        "- `columns`: The name of the categorical columns to be converted.\n",
        "- `drop_first`: Boolean to indicate onehot encoding (`False`) or dummy encoding (`True`).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgPhMhuP0ixh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# One hot encode cat variables\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRopXo88Ehsh",
        "colab_type": "text"
      },
      "source": [
        "**Feature Engineering**\n",
        "\n",
        "Generating new predictive features from existing features is an important aspect of machine learning. New features could be engineered using:\n",
        "\n",
        "- Binning numeric values _(e.g. `age_category` column from `age` column)._\n",
        "- Interaction of 2 columns _(e.g. `total_salary`/`tenure`)._\n",
        "- Features from domain knowledge.\n",
        "\n",
        "We learned while visualizing categorical columns that being subscribed to `OnlineSecurity`, `OnlineBackup`, `DeviceProtection`, and `TechSupport` tend to drive less churn. Let's visualize this further with a new feature called `in_ecosystem` which counts the number of services a given customer is subscribed to.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssdA-5wC0ixq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Re-add Churned to add to train and test\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44suakgKGO8R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check out header again\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unCbSHWjGQcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Service columns\n",
        "service_columns = ['OnlineSecurity_Yes', 'OnlineBackup_Yes', 'DeviceProtection_Yes', 'TechSupport_Yes']\n",
        "\n",
        "# Create in_ecosystem column\n",
        "\n",
        "\n",
        "# Visualize churn by number of services subscribed\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MKUJnDV0ix9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create feature that is 1 if 2 or more services subscribed, 0 otherwise\n",
        "\n",
        "\n",
        "# Apply the same on test_X\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K88S8T_s0iyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop target variable from training and test data again \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhtT-Dvu6q5p",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "<center><h1> Q&A 4</h1> </center>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISvRUbPh0iyC",
        "colab_type": "text"
      },
      "source": [
        "## **Modeling**\n",
        "\n",
        "Most machine learning models for classification aim at creating a decision boundary between data points to generate predictions. For example, here is a decision line where the target variable is whether tumor is benign or cancerous based on tumor height and width:\n",
        "\n",
        "<br>\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/datacamp/machine-learning-with-scikit-learn-live-training/blob/master/assets/decision_boundary.gif?raw=true?\" width=\"50%\">\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "#### **Using K-Nearest Neighbors to Generate Predictions**\n",
        "\n",
        "The K-Nearest Neighbor tries to find the label of unseen data by choosing the label of the `K` closest points to it. Using our cancerous/benign tumour example, K-Nearest Neighbor would behave like this:\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/datacamp/machine-learning-with-scikit-learn-live-training/blob/master/assets/knn.gif?raw=true?\" width=\"50%\">\n",
        "</p>\n",
        "\n",
        "Just like almost all algorithms on `sklearn` - the `KNeighborsClassifier()` needs to be instantiated and follows the `.fit()` $\\rightarrow$ `.predict()` paradigm as such:\n",
        "\n",
        "```\n",
        "# Import algorithm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Instantiate it\n",
        "knn = KNeighborsClassifier(n_neighbors = k)\n",
        "\n",
        "# Fit on training data\n",
        "knn.fit(train_X, train_Y)\n",
        "\n",
        "# Create predictions\n",
        "predictions = knn.predict(test_X)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wpNKPQbLERL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import K-Nearest Neighbor Classifier and accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Instantiate K Nearest Neighbors with 6 neighbors\n",
        "\n",
        "\n",
        "# Fit on training data\n",
        "\n",
        "\n",
        "# Create Predictions\n",
        "\n",
        "\n",
        "\n",
        "# Calculate accuracy score on testing data\n",
        "\n",
        "\n",
        "\n",
        "# Print test accuracy score rounded to 4 decimals\n",
        "print('Test accuracy:', round(test_accuracy, 4))\n",
        "print('\\nTrain accuracy:', round(train_accuracy, 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqT_B6F3Vyjd",
        "colab_type": "text"
      },
      "source": [
        "#### **Using Decision Trees and Random Forests to Generate Predictions**\n",
        "\n",
        "A **decision tree** is a recursive algorithm that sequentially asks if-else questions about the data using a set of cutoff points designed to maximize the purity (homogeneity) of the resulting data points. \n",
        "\n",
        "Taking the tumour example, this would mean asking a series of questions about tumour height and width to determine whether a tumour is cancerous or not. Splits are made so that the resulting data points are as homogeneous as possible to predict the class on unseen data.\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/datacamp/machine-learning-with-scikit-learn-live-training/blob/master/assets/decision_tree.png?raw=true\" width=\"60%\">\n",
        "</p>\n",
        "\n",
        "<br>\n",
        "\n",
        "Just like `KNeighborsClassifier()` - the `DecisionTreeClassifier()` also uses the `.fit()` $\\rightarrow$ `.predict()` paradigm.\n",
        "\n",
        "A **Random Forest** pools the predictions of many decision trees each fit on a random number of features and samples from training data and returns the most common class for each sample of test data.\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/datacamp/machine-learning-with-scikit-learn-live-training/blob/master/assets/forest.gif?raw=true\" width=\"70%\">\n",
        "</p>\n",
        "\n",
        "It can be used using the `RandomForestClassifier()` object - and also fits the `.fit()` $\\rightarrow$ `.predict()` paradigm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNQ5NpwR0iyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import relevant packages\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Instantiate decision tree and random forest classifiers\n",
        "\n",
        "\n",
        "\n",
        "# Fit decision tree and random forest on data\n",
        "\n",
        "\n",
        "\n",
        "# Create Predictions on test and train data using decision tree\n",
        "\n",
        "\n",
        "\n",
        "# Create Predictions on test and train data using random forest\n",
        "\n",
        "\n",
        "\n",
        "# Calculate test and train accuracy score on decision tree\n",
        "\n",
        "\n",
        "\n",
        "# Calculate test and train accuracy score on random forest\n",
        "\n",
        "\n",
        "\n",
        "# Print test accuracy score rounded to 4 decimals\n",
        "print('Tree test accuracy:', round(test_accuracy_tree, 4))\n",
        "print('Tree train accuracy:', round(train_accuracy_tree, 4))\n",
        "\n",
        "# Print test accuracy score rounded to 4 decimals\n",
        "print('\\nForest test accuracy:', round(test_accuracy_forest, 4))\n",
        "print('Forest train accuracy:', round(train_accuracy_forest, 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge_tgMvtmKFm",
        "colab_type": "text"
      },
      "source": [
        "#### **Overfitting, the bias-variance tradeoff and cross validation**\n",
        "\n",
        "Checking out the results of the decision tree and random forest classifiers, the training accuracy far exceeds the testing accuracy score, suggesting that the model is fitting really well (a bit too well) on the training data and does not generalize to unseen data. \n",
        "\n",
        "This is called overfitting and can be illustrated by highly complex decision boundary while fitting the model on the training data.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/datacamp/machine-learning-with-scikit-learn-live-training/blob/master/assets/overfittinig_new.gif?raw=true\" width=\"50%\">\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "**Model Variance**\n",
        "\n",
        "A model is said to have high variance if it creates an elaborate decision boundary around data points for different sets of training data. \n",
        "\n",
        "<ins> It can be diagnosed if **training accuracy** >>> **test accuracy**. </ins>\n",
        "\n",
        "\n",
        "**Model Bias**\n",
        "\n",
        "A model underfits the data, or is said to have high bias if the decision boundary does not fit the data - and generates non-accurate predictions on both training and testing data.\n",
        "\n",
        "<ins> It can be diagnosed if both **training accuracy** and **test accuracy** are low. </ins>\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/datacamp/machine-learning-with-scikit-learn-live-training/blob/master/assets/high_bias.png?raw=true\" width=\"60%\">\n",
        "</p>\n",
        "\n",
        "\n",
        "**Cross Validation**\n",
        "\n",
        "Cross validation is considered best practice for assessing a model's performance. It essentially divides the training data `n` times into a training sets and a hold out set - iteratively fitting the model on the training set and validating on the hold out set storing each validation result separately. Finally, the `n` results are pooled to get a mean validation score. \n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/datacamp/machine-learning-with-scikit-learn-live-training/blob/master/assets/cross_validation.png?raw=true\" width=\"60%\">\n",
        "</p>\n",
        "\n",
        "\n",
        "Cross-validation can be done by using the `cross_val_score()` in `sklearn` - it takes in as arguments the following:\n",
        "\n",
        "- The instantiated model in question.\n",
        "- The training data and label.\n",
        "- `cv`: The number of cross validation folds.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBOD-0Rb0iyH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import relevant modules \n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Instantiate decision tree\n",
        "\n",
        "\n",
        "# Get cross validation scores\n",
        "\n",
        "\n",
        "# Fit on training data and get predictions\n",
        "\n",
        "\n",
        "\n",
        "# Fit on data\n",
        "print(cv_scores)\n",
        "print(\"\\nMean cross-val score:\", round(np.mean(cv_scores), 4))\n",
        "print(\"\\nTest score:\", round(accuracy_score(y_pred, test_Y), 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL13w1Lb63rE",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "<center><h1> Q&A 5</h1> </center>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd7VDPtxtNFt",
        "colab_type": "text"
      },
      "source": [
        "#### **Hyperparameter Tuning and grid-search**\n",
        "\n",
        "Almost all algorithms have hyperparameters that can be tuned to fine-tune their performance, reduce over-fitting and better capture the patterns in the dataset. Having a good understanding and intuition of how algorithms work is essential to fully utilize hyperparameter tuning for the purposes of improving model performance and testing different modeling strategies. Here we will tune the `max_depth` and `max_features` hyperparameters of the decision tree classifier to improve its performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CkRhqTi0iyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get all parameters of a decision tree\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3FqjSV0t-CG",
        "colab_type": "text"
      },
      "source": [
        "**Tuning maximum depth**\n",
        "\n",
        "From the `sklearn` [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html):\n",
        "\n",
        "> The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than 2 samples.\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/datacamp/machine-learning-with-scikit-learn-live-training/blob/master/assets/max_depth.png?raw=true\" width=\"50%\">\n",
        "</p>\n",
        "\n",
        "\n",
        "The higher this number is, the more likely the model is to overfit. Let's try a `max_depth` of 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Uv9WCD40iyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import relevant modules\n",
        "from sklearn.tree import DecisionTreeClassifier \n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Instantiate a decision tree with max_depth = 4\n",
        "\n",
        "\n",
        "# Get cross validation scores\n",
        "\n",
        "\n",
        "# Fit on training data and get predictions\n",
        "\n",
        "\n",
        "\n",
        "# Print accuracy scores\n",
        "print(cv_scores)\n",
        "print(\"\\nMean cross-val score:\", round(np.mean(cv_scores), 4))\n",
        "print(\"\\nTest score:\", round(accuracy_score(y_pred, test_Y), 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUM2yP3-tyNR",
        "colab_type": "text"
      },
      "source": [
        "**Tuning maximum features**\n",
        "\n",
        "From the `sklearn` [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html):\n",
        "\n",
        "> The number of features to consider when looking for the best split.\n",
        "\n",
        "It could take many different values from:\n",
        "- `\"sqrt\"` so that `max_num_features = sqrt(num_features)`.\n",
        "- A float between 0 and 1 so that it is the percentage of features considered.\n",
        "- Or `int` - considering the exact number of features. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgLYoybR0iye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import relevant modules\n",
        "from sklearn.tree import DecisionTreeClassifier \n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Instantiate a decision tree with max_depth = 4 and max_features = 25\n",
        "\n",
        "\n",
        "# Get cross validation scores\n",
        "\n",
        "\n",
        "# Fit on training data and get predictions\n",
        "\n",
        "\n",
        "# Print accuracy scores\n",
        "print(cv_scores)\n",
        "print(\"\\nMean cross-val score:\", round(np.mean(cv_scores), 4))\n",
        "print(\"\\nTest score:\", round(accuracy_score(y_pred, test_Y), 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHvodiPqvQqy",
        "colab_type": "text"
      },
      "source": [
        "**Using grid-search**\n",
        "\n",
        "Grid-search is a hyperparameter tuning algorithm that sequentially goes through every possible combination of hyperparameter combination it is fed in space. For example, for hyperparameters `parameter 1` and `parameter 2` - it would mean testing out all possible combinations of their values:\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/datacamp/machine-learning-with-scikit-learn-live-training/blob/master/assets/grid-search.gif?raw=true\" width=\"50%\">\n",
        "</p>\n",
        "\n",
        "\n",
        "Grid-search can be done using the `GridSearchCV()` function - it takes in as arguments:\n",
        "\n",
        "- The model being used.\n",
        "- The possible parameters to test - inputted as a dictionary. \n",
        "- `cv`: The number of cross-validation folds.\n",
        "- `verbose`: More detailed output if `2`.\n",
        "\n",
        "**Note**: Grid-search can be very time-consuming if you are testing many different combinations using a complex learning model. [`RandomizedSearchCV()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) could be a better alternative.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cVmSKi50iyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grid\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate a decision tree classifier \n",
        "\n",
        "\n",
        "# Instantiate a GridSearchCV classifier with 10 fold cross-validation\n",
        "\n",
        "\n",
        "# Fit clf on training data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZMU6Utn0iyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate predictions and calculate accuracy error\n",
        "\n",
        "\n",
        "# Get best parameters and accuracy score\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETWTRB7P6tfQ",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "<center><h1> Q&A 6</h1> </center>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBPe6QH5Tehf",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<center><h1>Homework</h1> </center>\n",
        "\n",
        "Try to break the **80%** accuracy threshold on the test data.\n",
        "\n",
        "*Tips:* <br>\n",
        "\n",
        "- Use different models (Random Forest, logistic regression, SVM and more)\n",
        "- Try hyperparameter-tuning these models - make sure you read the sklearn - - documentation for each model.\n",
        "- Investigate engineering new features for your model.\n",
        "\n",
        "*Submission details:*<br>\n",
        "\n",
        "Share with us a code snippet with your output on LinkedIn, Twitter <br>\n",
        "Tag us `@DataCamp` with the hashtag `#datacamplive`<br>\n",
        "Or reach out on [Linkedin](https://www.linkedin.com/in/adelnehme/) or [Twitter](https://twitter.com/Adel_Nehme)\n",
        "\n"
      ]
    }
  ]
}